# SimLOB中存在的问题

Learning Representations of Limited Order Book for Financial Market Simulation

文章可以参考{cite:p}`li2024simlob`.

## 限价订单簿 LOB


## 1. 数据处理

### 1.1 数据生成中存在的问题

#### 1.1.1 生成ABM模型参数

在Maxe的{ref}`XML 配置文件 <XML_file>`中，除了{ref}`PGPS模型 <PGPS>`所需的6个参数之外，
还有一些（理应）可以影响模型数据生成的参数需要随机产生，以增强训练得到的神经网络的Generalization。

例如：

* `SetupAgent中`:`bidVolume`,`askVolume`,`bidPrice`,`askPrice`, 这四个参数是模型的起始状态(应该属于SimLOB模型中的上下文信息),如果在所有训练数据中都使用相同的参数作为起始值, 模型的泛用性必然遭到质疑, 特别是在面对下游任务为真实数据的情况时.
  
```{admonition} 讨论
:class: note
虽然通过数据预处理中的归一化和正则化处理后, 这些上下文信息在一定程度上能被消除, 但是ABM模型本身可能会基于这些上下文信息下订单, 特别是在比PGPS更复杂的ABM模型中.
```

* `Simulation`中: `random_seed`, 该参数决定了PGPS模型的随机性, 相同参数在不同随机种子下产生的结果可能大相径庭(还需要对Maxe中的PGPS模型进行更深入的研究才可以确定). 但如果考虑模型的随机性, 那么下游的校准任务将变得无比复杂, 这可以在后续的{ref}`Simulation-based Inference  <SBI>` 研究中做进一步讨论.

### 1.2 数据清洗时存在的问题

#### 1.2.1 错误数据
TheSimulator(或者说PGPS模型)并不保证所有生成的数据都是正确的, 即使所有参数都在指定的参数范围之内. 其中一个最典型的错误就是在LOB数据中, 价(量)产生负值, 这在实际中是不可能发生的. 在一次模拟中如果出现负值, 意味着这条数据(在SimLOB实验中为50000个时间点)后面的值都有问题.

SimLOB训练数据是由单条数据按照100时间点长度进行切分的(不重叠的滑动窗口法), 一条错误的模拟最多会向训练数据集中引入500条错误的训练数据. 以现在SimLOB训练数据集原始记录来看, 至少有 $ 66/2000=3.3% $ 条这样的数据. 从训练过程来看, 这些数据点会导致模型不稳定, 并且数量符合前面计算的`3.3%`. 如[下图](error_data)所示:

```{figure} ./imgs/error_data.png
:name: error_data

错误数据对训练造成的影响.
```

从图中可以看出, 在某些batch, 训练过程中的Loss会明显高于其他batch. 并且这些位置不因训练模型的不同而有区别. (因为在DataLoader中使用了相同的随机种子,因此这些出错的batch位置会相同.)


```{admonition} 改进方法:
:class: tip
* 在数据清洗中, 如果数据出现了错误, 就删除整条数据. 
* 记录出错的参数组合(如果后续的其他工作可能用得上).
```


#### 1.2.2 低质量数据
从上面的错误数据来看, Maxe除了生成错误数据外, 肯定还会生成不少低质量的数据, 这些数据虽然符合作为金融数据的约束条件, 但是与实际的金融数据可能相差甚远, 这样的训练数据可能会影响模型的校准能力.

```{admonition} 讨论
:class: note

有几点可以再仔细研究:
1. 如何判断不良数据? (Stylized Factors?)
2. 这些数据与金融市场的异常数据有什么区别和联系? 
    * 是否可以利用这些数据训练模型, 让模型可以检测异常?
```

#### 1.2.3 改善数据集的效果
```{figure} ./imgs/default_data_vs_cleaned_data.png
:name: cleaned_data

对已有的CSV数据做初步清理后的训练结果对比
```
```{figure} ./imgs/default_data_vs_cleaned_data_valid.png
:name: cleaned_data_valid

对已有的CSV数据做初步清理后的验证结果对比
```
从这组对比实验可以看出, 只是清理数据集,对模型的最终效果提升不大, 但是训练的收敛速度更快, 训练过程更平稳(中间没有产生很大的杂峰).


```{figure} ./imgs/default_data_vs_cleaned_data_improved.png
:name: cleaned_data_imporved_model

在新训练集上进一步改善模型的训练结果对比
```

```{figure} ./imgs/default_data_vs_cleaned_data_improved_valid.png
:name: cleaned_data_imporved_model_valid

在新训练集上进一步改善模型的验证结果对比
```
以上对比实验,可以看出, 清理过的数据集可以更好的支持更小规模的trans_encoder_layer, 相比在之前数据集上的原始模型的性能提升幅度甚至高达$30\%$. 

```{admonition} 初步结论
:class: tip

这组对比试验可以更清晰的看到,在新数据集上, 模型的训练波动更小, 收敛更快.
```

### 1.3 数据预处理时存在的问题
#### 1.3.1 训练数据分割
现在的训练集是通过分割Maxe产生的原始LOB数据产生, 分割方法是非重叠的滑动窗口法, 将原始时间步为50000的时序数据按100每段进行切分, 每条原始数据获得500条训练数据.
```{admonition} 冷启动问题
:class: warning

从前面对Maxe模拟器的xml文件的了解可以知道, 每一次模拟产生数据时都有一个上下文状态, 在SimLOB中, 该状态为:
    * bidVolume="10"
    * askVolume="10"
    * bidPrice="7514.9"
    * askPrice="7516.1"

一个直观的分析就是,这个LOB状态非常单薄, 如果在启动时产生一个超过10手的市价买单, 卖方就会被买空, 可能导致下一次卖单没有参考价格(系统认为目前最佳卖价为正无穷); 反之, 如果起始时产生超过10手的市价卖单, 买房会被买空, 导致下次买单没有参考价格(系统认为目前最佳买家为0). 

```


```{admonition} 解决方案
:class: tip

一个可行的方案是, 抛开原始数据的起始部分(比如前2000个时间步), 对剩下的部分进行采样.

采样的方案也可以不局限于非重叠的滑动窗口法, 可能以某个百分比重叠的滑动窗口更能抓住时序数据中的自相关属性.

```

#### 1.3.2 标准化方法
在SimLOB中, 对数据按列进行标准化, 具体方法如下:

```python
def standardize_cols(df, mean_list, std_list):

  for i,col in enumerate(df.columns):
      
    # 如果是时间列无法进行标准化！
    if col == 'time':
        continue
    arr = df[col].values
    arr = (arr - mean_list[i]) / std_list[i] 
    df[col] = arr

  return df
```

```{admonition} 讨论
:class: note
* **Z-score**是一种通用的机器学习数据预处理方法, 但是这样的方法在金融时序数据中是否有意义? 以价数据为例, 实际上订单只会造成某价格上量的上下波动, 但是在LOB数据中, 量数据会跟着价格的档位跳动. 
    * 比如, 1档买价 10.1,量10, 2档买价 10.2, 量1000, 3档买价10.3, 10, 当有卖单10.1卖出10手, 当前的1档买价变成10.2, 量1000, 2档买价10.3, 10; 虽然在系统中只成交了一笔非常小的订单,但是造成了LOB数据的巨幅改变. 
    * 如果以列作为数据Feature(以列为单位进行各种处理), 并用于训练模型,很可能会导致: 
        1. 训练过程不稳定
        2. 模型的输入输出缺乏可解释性
    * 如何合适地处理LOB数据, 对其进行符合现实情况地标准化, 还值得仔细研究.
* 是否应该按照价/量进行统一的标准化?
* 是否应该先对每一次模拟产生的LOB数据进行一次标准化, 再在训练数据集中对数据进行一次统一的标准化? 

```


```{admonition} 按列标准化
:class: warning
在 generate_data.py模块的整体标准化方法中, 关于'MPV'的整体标准化方法没有实现, 现在实验采用的是对于每次模拟产生的数据进行**按列标准化**之后进行分割并加入训练集. 
```

考虑到一次模拟产生的数据, 其上下文是一致的, 使用按单次模拟的价/量分别进行标准化是一个更好的选择.

### 1.4 其他细节

#### 1.4.1 模拟的效率

**问题所在:**

在使用Maxe编译得到的`TheSimulator`仿真器生成数据时, 有两个重要步骤:

    1. 产生{ref}`XML 配置文件 <XML_file>`
    2. 通过`TheSimulator`读取该文件, 进行模拟后将输出保存在`.csv`文件中

这两个步骤都需要对磁盘进行读写, 在大量生成数据时会产生两个问题:

```{admonition} 问题:
:class: warning

1. 读写效率: 对物理磁盘进行读写, I/O的效率相比内存读写要低很多, 特别是当Maxe采用多进程/线程的方式模拟多个Agent时

2. 在数据生成过程中循环使用相同的文件名保存xml配置文件, 单进程时不会影响, 如果多进程同时更新/读取同一个配置文件时会导致出错/重复. 
```

```{admonition} 解决办法:
:class: tip

使用python提供的虚拟文件系统包:`tempFiles`

  * 在内存中模拟文件句柄并返回给生成xml文件进程, 将xml配置写入虚拟文件, 同时在xml文件的输出配置项中写入另一个虚拟文件句柄.
  * 通过TheSimulator进行仿真, 数据会被写入上述虚拟文件中.
  * 这样做的好处有:
    * 虚拟文件名会自动进行hash不会出现相同文件名的情况.
    * 虚拟文件在内存中读写速度更快.
    * 可以直接将模拟结果读入pandas的DataFrame不进行中转.
    * 内存数据会按照系统资源使用情况被自动清理(也可以手动清理.)
```

## 2. 方法
### 2.2 Train Loss设计
在SimLOB中使用MESLoss作为自编码器的重构误差,用该Loss训练AE模型. 
```{admonition} Loss 问题
:class: error

同前面{ref}`关于LOB数据标准化的讨论 <lob_normalize>', 两个LOB数据之间的微小差异,可能导致MESLoss产生巨大差别.如图所示:

```

### 2.3 优化方法
在SimLOB工作中, 使用PSO对PGPS模型的6个参数进行优化, 优化的目标函数是:

\begin{align}
MSE(Encoder(RealLOB),Encoder(SimLOB))
\end{align}

```{admonition} 实验中的问题
:class: warning

以下问题可能是因为我重复实验室使用了不一样的参数进行优化得到的结果获得, 具体是什么问题还得完全复现原实验后再分析.

**问题1:**

已经训练好的Encoder在训练Loss没有使用正则化的情况下,(以一次运行为例) 输出的潜变量的均值约为$0.005$, 而其标准差约为$0.08$, 这一结果说明潜变量的分布非常集中(在图像上表现为在$0.005$附近细高的峰), 不知道在一般的AutoEncoder模型中, 这种分布是否属于正常现象, 待进一步确认.

**问题2:**

PSO方法种群大小为$40$, 最大优化代数为$100$, (以一次运行为例)优化开始时的Loss值为$0.043$, 在优化完成后该值为$0.039$, 相比随机的初始化值只优化了不到10%. 这可能是下面原因导致:

1. 该问题的优化难度非常大, 以致该参数设置下的PSO无法胜任
2. Encoder本身对原数据的表征不充分.

* 具体属于什么原因, 还有待下一步研究.

**问题3:**

* 结合上面两点, PSO的MSE损失函数代表的意义是每个元素差异的平均值, 在潜变量输出方差为$0.08$的前提下, 其优化后的损失值为$0.039$, 从直观上来说, 就是合成数据与待校准数据潜变量之间的差值达到了其绝对值的$50\%$, 这很难说PSO已经做了很好的优化了.
```


结合实验结果来看, 使用`PSO`方法, 种群大小设置为$40$, 迭代$100$轮, 其结果与随机产生4000组随即参数后选择其中最好的一组, 没有本质区别.

### 2.4 神经网络模型设计
SimLOB使用了多层神经网络, 如图`ref<SimLOB Structure>`所示:
(SimLOB Structure)=
![SimLOB神经网络结构](imgs/nn-structure.png)

:TODO

## 3. 实验设置



## 4. 代码实现

### 4.1 优化部分
```{admonition} 校准时的数据生成过程实现问题:
:class: error

* 在优化过程中, 调用TheSimulator生成LOB文件, 这个文件会写入磁盘, 然后由下一行代码将该文件内容读入pandas.DataFrame中. 这会造成一个读写的时间差, 如果是系统中原本不存在的csv, 这个过程必然会报错, 提示文件不存在.
* 该实现的解决办法是循环使用几个系统中已存在文件名来存储数据, 但这样做就可能出现常见的同时读写问题, 大概率是读到之前的csv文件( 如果写的过程中读文件可能发生不可预料的错误 ), 如果文件数量一次迭代所需的数量相同, 基本上可以认为 `t`时刻优化用的是`t-1`时刻的数据, 约等于少一次优化. 如果数量少于所需数量, 可能严重影响优化过程.
```

```{admonition} 实现效率:
:class: warning

* 读写物理文件系统的效率问题.
* `to_latent`方法计算Loss时使用了to('cpu')将数据迁移到cpu上再用自己实现的mse方法进行计算, 相比直接在gpu上, 利用内置函数算完mse,再将结果迁移回cpu(供PSO在cpu上使用), 效率会大打折扣.
* 在使用Encoder计算latent的时候没有使用`with torch.no_grad()`或其他类似的操作让系统不记录梯度值, 效率会有所降低.
* `data_classification_tensor`方法实现的实际上就是一个reshape方法, 自己重写效率很低.

```

## 5. 结果讨论

