# SimLOB中存在的问题

## 1. 数据处理

### 1.1 数据生成中存在的问题

#### 1.1.1 生成ABM模型参数

在Maxe的{ref}`XML 配置文件 <XML_file>`中，除了{ref}`PGPS模型 <PGPS>`所需的6个参数之外，
还有一些（理应）可以影响模型数据生成的参数需要随机产生，以增强训练得到的神经网络的Generalization。

例如：

* `SetupAgent中`:`bidVolume`,`askVolume`,`bidPrice`,`askPrice`, 这四个参数是模型的起始状态(应该属于SimLOB模型中的上下文信息),如果在所有训练数据中都使用相同的参数作为起始值, 模型的泛用性必然遭到质疑, 特别是在面对下游任务为真实数据的情况时.
  ```{note}
  虽然通过数据预处理中的归一化和正则化处理后, 这些上下文信息在一定程度上能被消除, 但是ABM模型本身可能会基于这些上下文信息下订单, 特别是在比PGPS更复杂的ABM模型中.
  ```
* `Simulation`中: `random_seed`, 该参数决定了PGPS模型的随机性, 相同参数在不同随机种子下产生的结果可能大相径庭(还需要对Maxe中的PGPS模型进行更深入的研究才可以确定). 但如果考虑模型的随机性, 那么下游的校准任务将变得无比复杂, 这可以在后续的{ref}`Simulation-based Inference  <SBI>` 研究中做进一步讨论.

### 1.2 数据清洗时存在的问题

#### 1.2.1 错误数据
TheSimulator(或者说PGPS模型)并不保证所有生成的数据都是正确的, 即使所有参数都在指定的参数范围之内. 其中一个最典型的错误就是在LOB数据中, 价(量)产生负值, 这在实际中是不可能发生的. 在一次模拟中如果出现负值, 意味着这条数据(在SimLOB实验中为50000个时间点)后面的值都有问题.

SimLOB训练数据是由单条数据按照100时间点长度进行切分的(不重叠的滑动窗口法), 一条错误的模拟最多会向训练数据集中引入500条错误的训练数据. 以现在SimLOB训练数据集原始记录来看, 至少有66/2000 条这样的数据. 从训练过程来看, 这些数据点会导致模型不稳定. 如[下图](error_data)所示:

```{figure} ./imgs/error_data.png
:name: error_data

错误数据对训练造成的影响.
```


### 1.3 数据预处理时存在的问题

### 1.4 数据分割时存在的问题

### 1.5 其他细节问题

#### 1.5.1 生成的效率

**问题所在:**

在使用Maxe编译得到的`TheSimulator`仿真器生成数据时, 有两个重要步骤:

    1. 产生{ref}`XML 配置文件 <XML_file>`
    2. 通过`TheSimulator`读取该文件, 进行模拟后将输出保存在`.csv`文件中

这两个步骤都需要对磁盘进行读写, 在大量生成数据时会产生两个问题:

    ```{warning}
       1. 读写效率: 对物理磁盘进行读写, I/O的效率相比内存读写要低很多, 特别是当Maxe采用多进程/线程的方式模拟多个Agent时,
       2. 在数据生成过程中循环使用相同的文件名保存xml配置文件, 单进程时不会影响, 如果多进程同时更新/读取同一个配置文件时会导致出错/重复. 
    ```
**解决办法:**

使用python提供的虚拟文件系统包:`tempFiles`

  * 在内存中模拟文件句柄并返回给生成xml文件进程, 将xml配置写入虚拟文件, 同时在xml文件的输出配置项中写入另一个虚拟文件句柄.
  * 通过TheSimulator进行仿真, 数据会被写入上述虚拟文件中.
  * 这样做的好处有:
    * 虚拟文件名会自动进行hash不会出现相同文件名的情况.
    * 虚拟文件在内存中读写速度更快.
    * 可以直接将模拟结果读入pandas的DataFrame不进行中转.
    * 内存数据会按照系统资源使用情况被自动清理(也可以手动清理.)
  




## 2. 方法详情

## 3. 实验设置

## 4. 代码实现

## 5. ABM模型
