{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主要模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# import os \n",
    "# os.environ['CUDA_VISIBLE_DEVICE'] = '0'\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "operator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(train_path, transform\u001b[38;5;241m=\u001b[39mdata_transform)\n\u001b[1;32m      4\u001b[0m val_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(val_path, transform\u001b[38;5;241m=\u001b[39mdata_transform)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.8/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.8/site-packages/torchvision/_meta_registrations.py:164\u001b[0m\n\u001b[1;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         ),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_custom_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision::nms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should have 4 elements in dimension 1, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.8/site-packages/torch/library.py:440\u001b[0m, in \u001b[0;36minner\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpl_abstract\u001b[39m(qualname, func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, lib\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Register an abstract implementation for this operator.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    An \"abstract implementation\" specifies the behavior of this operator on\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Tensors that carry no data. Given some input Tensors with certain properties\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    (sizes/strides/storage_offset/device), it specifies what the properties of\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    the output Tensors are.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    The abstract implementation has the same signature as the operator.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    It is run for both FakeTensors and meta tensors. To write an abstract\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m    implementation, assume that all Tensor inputs to the operator are\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m    regular CPU/CUDA/Meta tensors, but they do not have storage, and\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    you are trying to return regular CPU/CUDA/Meta tensor(s) as output.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    The abstract implementation must consist of only PyTorch operations\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    (and may not directly access the storage or data of any input or\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    intermediate Tensors).\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    This API may be used as a decorator (see examples).\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    For a detailed guide on custom ops, please see\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m    https://docs.google.com/document/d/1W--T6wz8IY8fOI0Vm8BF44PdBgs283QvpelJZWieQWQ/edit\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m        >>> import torch\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m        >>> import numpy as np\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m        >>> from torch import Tensor\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m        >>> # Example 1: an operator without data-dependent output shape\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m        >>> torch.library.define(\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m        >>>     \"mylib::custom_linear\",\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m        >>>     \"(Tensor x, Tensor weight, Tensor bias) -> Tensor\")\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.impl_abstract(\"mylib::custom_linear\")\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m        >>> def custom_linear_abstract(x, weight):\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m        >>>     assert x.dim() == 2\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m        >>>     assert weight.dim() == 2\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m        >>>     assert bias.dim() == 1\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m        >>>     assert x.shape[1] == weight.shape[1]\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m        >>>     assert weight.shape[0] == bias.shape[0]\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m        >>>     assert x.device == weight.device\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m        >>>     return (x @ weight.t()) + bias\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03m        >>> # Example 2: an operator with data-dependent output shape\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m        >>> torch.library.define(\"mylib::custom_nonzero\", \"(Tensor x) -> Tensor\")\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.impl_abstract(\"mylib::custom_nonzero\")\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m        >>> def custom_nonzero_abstract(x):\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m        >>>     # Number of nonzero-elements is data-dependent.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m        >>>     # Since we cannot peek at the data in an abstract impl,\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m        >>>     # we use the ctx object to construct a new symint that\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m        >>>     # represents the data-dependent size.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m        >>>     ctx = torch.library.get_ctx()\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m        >>>     nnz = ctx.new_dynamic_size()\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m        >>>     shape = [nnz, x.dim()]\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m        >>>     result = x.new_empty(shape, dtype=torch.int64)\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m        >>>     return result\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;124;03m        >>>\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m        >>> @torch.library.impl(\"mylib::custom_nonzero\", \"cpu\")\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m        >>> def custom_nonzero_cpu(x):\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m        >>>     x_np = x.numpy()\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m        >>>     res = np.stack(np.nonzero(x_np), axis=1)\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m        >>>     return torch.tensor(res, device=x.device)\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     source \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_library\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_source(_stacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    449\u001b[0m     frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(_stacklevel)\n",
      "File \u001b[0;32m~/miniconda3/envs/sbi/lib/python3.8/site-packages/torch/_library/abstract_impl.py:30\u001b[0m, in \u001b[0;36mAbstractImplHolder.register\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl_abstract(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an abstract impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl_abstract(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl_abstract.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "train_data = datasets.ImageFolder(train_path, transform=data_transform)\n",
    "val_data = datasets.ImageFolder(val_path, transform=data_transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
